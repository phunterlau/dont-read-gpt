import os
import openai
import re
import backoff
from functools import partial

openai.api_key = os.environ['OPENAI_KEY']
client = openai.OpenAI(api_key=os.environ['OPENAI_KEY'])

def is_chinese(text):
    if any(u'\u4e00' <= c <= u'\u9fff' for c in text):
        return True
    return False

# Retry with exponential backoff just in case OpenAI API is temporarily unavailable
@backoff.on_exception(
    partial(backoff.expo, max_value=50),
    (openai.RateLimitError, openai.APIError, openai.APIConnectionError),
)
def gen_gpt_completion(prompt, temp=0.0, engine="text-davinci-003", max_tokens=100, 
                       top_p=1, frequency_penalty=0, presence_penalty=0,):
    response = client.completions.create(
        engine="text-davinci-003",
        prompt=prompt,
        max_tokens=max_tokens,
        temperature=0.5,
        top_p=1,
        stop = None,
        #frequency_penalty=0.5,
        #presence_penalty=0.5,
    )
    return response

# Use gpt-40o for longer text and chat completion
#@backoff.on_exception(
#    partial(backoff.expo, max_value=2),
#    (openai.RateLimitError, openai.APIError, openai.APIConnectionError),
#)
def gen_gpt_chat_completion(system_prompt, user_prompt, temp=0.0, engine="gpt-4o", max_tokens=1024,
                            top_p=1, frequency_penalty=0, presence_penalty=0,):
    
    response = client.chat.completions.create(
                    model="gpt-4o",
                    messages=[{"role":"system", "content":system_prompt},
                            {"role":"user", "content":user_prompt}],
                    temperature=1,
                    max_tokens=max_tokens,
                    top_p=1,
                    frequency_penalty=0,
                    presence_penalty=0)
    return response

def generate_summary(text_snippet, summary_type='general'):

    max_input_words = 100000
    max_input_words_chinese = 50000
    max_output_tokens = 1024
    total_tokens = 4097  # Assuming an average of 2 tokens per word

    if total_tokens > 4097:
        raise ValueError("The total number of tokens (input + output) must not exceed 4097.")

    # Truncate the text snippet if it's too long
    words = text_snippet.split()
    # for non-English text, we need to use a different method to split the text into words and fit the token size
    if is_chinese(text_snippet):
        text_snippet = text_snippet[:max_input_words_chinese]
    elif len(words) > max_input_words:
        words = words[:max_input_words]
        text_snippet = " ".join(words)

    #arxiv_paper_prompt = f'Please provide a summary of the following ArXiv paper\'s title and abstract within 450 chars, addressing the following aspects:\n1. Main problem or research question\n2. Key methodology or approach\n3. Main results or findings\n4. Comparison to previous work or state of the art\n5. Potential applications or implications\n6. Limitations or future work\n\n{text_snippet}\n\nSummary:'
    #github_repo_prompt = f'Please provide a summary of the following GitHub repository README.mdÂ· within 450 chars, addressing the following aspects:\n1. Purpose of the project\n2. Key features and benefits\n3. Technology stack or programming languages used\n4. Dependencies or prerequisites\n5. Installation, setup, and usage instructions\n6. Maintenance and main contributors\n7. Known limitations or issues\n8. Project license and usage restrictions\n9. Contribution guidelines\n10. Additional documentation or resources\n\n{text_snippet}\n\nSummary:'
    #general_prompt = f'Please provide a concise summary of the following text within 380 chars, addressing the following aspects:\n1. Main topic or subject\n2. Core arguments or points\n3. Significant findings, results, or insights\n4. Comparisons or contrasts with other ideas or studies\n5. Implications or potential applications\n\n{text_snippet}\n\nSummary:'
    #arxiv_paper_prompt = f'You are a scientific researcher. Please provide a concise summary of the following ArXiv paper\'s title and abstract, limit 100 words, addressing the following aspects if available:\n1. Main problem or research question\n2. Key methodology or approach\n3. Main results or findings\n4. Comparison to previous work or state of the art\n5. Potential applications or implications\n6. Limitations or future work'
    #github_repo_prompt = f'You are a scientific researcher. Please provide a concise summary of the following GitHub repository README.md, limit 100 words, addressing the following aspects if available:\n1. Purpose of the project\n2. Key features and benefits\n3. Technology stack or programming languages used\n4. Dependencies or prerequisites\n5. Installation, setup, and usage instructions\n6. Maintenance and main contributors\n7. Known limitations or issues\n8. Project license and usage restrictions\n9. Contribution guidelines\n10. Additional documentation or resources'
    #youtube_prompt = f'You are a scientific researcher. Expert in technical and research content analysis: Summarize the video transcript (150 words max). Cover if available: 1. Main topic. 2. Key concepts/technologies. 3. Primary arguments/points. 4. Significant findings/conclusions. 5. Methodologies/approaches. 6. Examples/case studies. 7. Relation to current trends/knowledge. 8. Counterpoints/alternatives. 9. Future implications/applications. 10. Key quotes/statements. 11. Limitations/weaknesses. 12. Additional resources. 13. Key takeaways.'
    arxiv_paper_prompt = (
    'You are a scientific researcher. Provide a focused 100-word summary of this ArXiv paper that emphasizes:\n'
    '1. Key Innovation: What is novel or groundbreaking about this work?\n'
    '2. Core Contribution: What specific technical/methodological advancement does it make?\n'
    '3. Impact: How does this advance the field or solve an important problem?\n'
    '4. Significance: Why does this matter to the research community or broader applications?\n'
    'Format: Start with the innovation, then explain the contribution, followed by impact and significance.\n'
    'Note: Be specific about technical details while maintaining clarity. Prioritize describing what makes this work unique.'
    )
    github_repo_prompt = (
    'You are a scientific researcher. Please provide a concise summary of the following GitHub repository README.md, limit 100 words. Include only the aspects that are available:\n'
    '1. Purpose of the project.\n'
    '2. Key features and benefits.\n'
    '3. Technology stack or programming languages used.\n'
    '4. Dependencies or prerequisites.\n'
    '5. Installation, setup, and usage instructions.\n'
    '6. Maintenance and main contributors.\n'
    '7. Known limitations or issues.\n'
    '8. Project license and usage restrictions.\n'
    '9. Contribution guidelines.\n'
    '10. Additional documentation or resources.\n'
    'Note: Omit any aspects not mentioned in the README.md.'
    )
    youtube_prompt = (
    'You are a technical content analyst. Provide a focused 100-word summary of this video transcript that emphasizes:\n'
    '1. Core Message: What is the main insight or key learning?\n'
    '2. Technical Details: What specific methods, tools, or technologies are discussed?\n'
    '3. Practical Application: How can viewers apply this information?\n'
    '4. Expert Insights: What unique perspectives or expert knowledge is shared?\n'
    'Format: Begin with the core message, then explain technical aspects, followed by practical applications and expert insights.\n'
    'Note: Prioritize actionable insights and technical depth while maintaining clarity. Focus on what makes this content valuable to practitioners.'
    )

    huggingface_model_prompt = (
    'You are an AI expert. Please provide a concise summary of the Hugging Face model page, limit 100 words. Include only the aspects that are available on the page:\n'
    '1. Model Overview: A brief description and primary function of the model.\n'
    '2. Architecture: Type of neural network used.\n'
    '3. Training Data: Key datasets and preprocessing details, if mentioned.\n'
    '4. Performance: Important metrics and benchmark results, if provided.\n'
    '5. Use Cases: Typical applications and examples of implementation, if listed.\n'
    '6. Fine-tuning: Information on adaptability for specific tasks, if available.\n'
    '7. Limitations and Biases: Any known constraints and ethical considerations, if discussed.\n'
    '8. Accessibility: License information and model availability, if specified.\n'
    'Note: Exclude any aspects that are not explicitly mentioned on the model page.'
    )
    ipython_notebook_prompt = (
    'You are a data scientist. Please provide a concise summary of the following IPython notebook, limit 100 words. Include only the aspects that are available:\n'
    '1. Main objectives or goals of the notebook.\n'
    '2. Key data analysis or computational concepts demonstrated.\n'
    '3. Significant findings or results derived from the notebook.\n'
    '4. Code and methodologies used and their significance.\n'
    '5. Any visualizations or graphical representations and their insights.\n'
    '6. Conclusions or potential applications of the notebook\'s content.\n'
    '7. Limitations or areas for further exploration, if mentioned.\n'
    'Note: Focus only on the aspects explicitly included in the notebook.'
    )
    general_prompt = (
    'You are a scientific researcher. Please provide a concise summary of the following text, limited to 100 words. Include only the aspects that are available:\n'
    '1. Main topic or subject.\n'
    '2. Core arguments or points.\n'
    '3. Significant findings, results, or insights.\n'
    '4. Comparisons or contrasts with other ideas or studies.\n'
    '5. Implications or potential applications.\n'
    'Note: Focus only on the aspects explicitly mentioned in the text.'
    )
    prompts = {
        'arxiv': arxiv_paper_prompt,
        'github': github_repo_prompt,
        'youtube': youtube_prompt,
        'huggingface': huggingface_model_prompt,
        'ipython': ipython_notebook_prompt,
        'general': general_prompt,
    }

    system_prompt = prompts.get(summary_type, prompts['general'])
    user_prompt = text_snippet

    try:
        response = gen_gpt_chat_completion(system_prompt, user_prompt, max_tokens=max_output_tokens)
        summary = response.choices[-1].message.content.strip()
        return summary
    except openai.BadRequestError as e:
        if "maximum context length" in str(e):
            error_message = "The input is too long. Please reduce the length and try again."
        else:
            error_message = "An error occurred: " + str(e)
        return error_message

def generate_embedding(text_snippet):
    #embedding = openai.Embedding.create(
    #    input=text_snippet, model="text-embedding-ada-002"
    #)["data"][0]["embedding"]
    embedding = client.embeddings.create(
        input=text_snippet[:8192], model="text-embedding-ada-002"
    ).data[0].embedding
    return embedding

def extract_keywords_from_summary(summary):
    prompt = (
        f"Given the following summary, identify important keywords or phrases that would be suitable for creating internal links in Obsidian. These keywords should represent the main concepts, topics, or entities in the summary.\n\n"
        f"Please provide a list of Obsidian Keywords, separated by commas:"
    )
    #response = openai.Completion.create(
    #    engine="text-davinci-003",
    #    prompt=prompt,
    #    max_tokens=100,
    #    n=1,
    #    stop=None,
    #    temperature=0.5,
    #)
    system_prompt = prompt
    user_prompt = summary
    #response = gen_gpt_completion(prompt, max_tokens=100)
    try:
        response = gen_gpt_chat_completion(system_prompt, user_prompt, max_tokens=256)

        #keywords = response.choices[0].text.strip().split(', ')
        keywords = response.choices[-1].message.content.strip().split(', ')
        return keywords
    except openai.BadRequestError as e:
        # Check if the error is due to exceeding maximum context length
        if "maximum context length" in str(e):
            error_message = "The input is too long. Please reduce the length and try again."
        else:
            error_message = "An error occurred: " + str(e)
        return error_message

def download_youtube_transcript(video_id):
    """
    Downloads a YouTube video transcript using youtube_transcript_api.
    Returns the transcript text or error message.
    
    Args:
        video_id (str): YouTube video ID (e.g., 'dQw4w9WgXcQ' from youtube.com/watch?v=dQw4w9WgXcQ)
    
    Returns:
        str: Transcript text or error message
    """
    try:
        from youtube_transcript_api import YouTubeTranscriptApi
        
        # Get transcript
        transcript_list = YouTubeTranscriptApi.get_transcript(video_id)
        
        # Combine transcript segments into full text
        transcript_text = ' '.join([item['text'] for item in transcript_list])
        
        return transcript_text
        
    except Exception as e:
        return f"Error downloading transcript: {str(e)}"

def summary_to_obsidian_markdown(summary, keywords):
    not_found_keywords = []

    for keyword in keywords:
        pattern = re.compile(re.escape(keyword), re.IGNORECASE)
        matches = pattern.findall(summary)
        if matches:
            summary = pattern.sub(f"[[{keyword}]]", summary)
        else:
            not_found_keywords.append(keyword)
    
    if not_found_keywords:
        additional_keywords = ", ".join(f"[[{keyword}]]" for keyword in not_found_keywords)
        summary += f"\n\nAdditional Keywords: {additional_keywords}"
    
    return summary

def test_obsidian_markdown():
    summary = "This is a summary of a paper about the use of AI in the field of medicine. The paper discusses the use of AI to diagnose diseases and predict the effectiveness of treatments. The paper also discusses the use of AI to predict the effectiveness of treatments for diseases such as cancer and diabetes. The paper also discusses the use of AI to predict the effectiveness of treatments for diseases such as cancer and diabetes. The paper also discusses the use of AI to predict the effectiveness of treatments for diseases such as cancer and diabetes. The paper also discusses the use of AI to predict the effectiveness of treatments for diseases such as cancer and diabetes. The paper also discusses the use of AI to predict the effectiveness of treatments for diseases such as cancer and diabetes. The paper also discusses the use of AI to predict the effectiveness of treatments for diseases such as cancer and diabetes."
    keywords = extract_keywords_from_summary(summary)
def test_obsidian_markdown():
    summary = "This is a summary of a paper about the use of AI in the field of medicine. The paper discusses the use of AI to diagnose diseases and predict the effectiveness of treatments. The paper also discusses the use of AI to predict the effectiveness of treatments for diseases such as cancer and diabetes. The paper also discusses the use of AI to predict the effectiveness of treatments for diseases such as cancer and diabetes. The paper also discusses the use of AI to predict the effectiveness of treatments for diseases such as cancer and diabetes. The paper also discusses the use of AI to predict the effectiveness of treatments for diseases such as cancer and diabetes. The paper also discusses the use of AI to predict the effectiveness of treatments for diseases such as cancer and diabetes. The paper also discusses the use of AI to predict the effectiveness of treatments for diseases such as cancer and diabetes."
    keywords = extract_keywords_from_summary(summary)
    result = summary_to_obsidian_markdown(summary, keywords)
    print(keywords, result)

if __name__ == '__main__':
    test_obsidian_markdown()
